# KNN 
## 지도학습의 대표적인 머신 러닝 방법:
 - 분류: 미리 정의된, 가능성 있는 여러 클래스 레이블(class label) 중 하나를
예측하는 것
- 회귀: 연속적인 숫자 또는 부동소수점수 (실수)를 예측하는 것
## KNN:
  #### 주변 k 개의 자료의 클래스(class) 중 가장 많은 클래스로 특정 자료를 분류하는 방식
   - 게으른 학습(lazy learner) 또는 사례중심학습(instance-based learning)
   - 데이터의 차원이 증가하면 차원의 저주(curse of dimension) 문제가 발생함 = 성능 저하  
  
  #### 탐색할 이웃 수(k)와 거리 측정 방법: 
   - k가 작을 경우 데이터의 지역적 특성을 지나치게 반영하여 과접합(overfitting) 발생
   - 반대로 매우 클 경우 모델이 과하게 정규화 (underfitting) 발생

  #### KNN의 K가 가지는 의미: 
   - 새로운 자료에 대해 근접치 K의 개수에 따라 Group이 달리 분류됨 (다수결 방식 , 가중 합 방식)
  
  #### 장점: 
   - 학습데이터 내에 끼어있는 노이즈의 영향을 크게 받지 않음
   - 학습데이터 수가 많다면 꽤 효과적인 알고리즘
   - 마할라노비스 거리와 같이 데이터의 분산을 고려할 경우 매우 강건(robust)한 방법론

  #### 단점: 
   - 최적 이웃의 수(k)와 어떤 거리 척도(distance metric)가 분석에 적합한지 불분명해
   - 데이터 각각의 특성에 맞게 연구자가 임의로 선정해야 함
   - best K는 데이터 마다 다르기 때문에 탐욕적인 방식(Grid Search)으로 탐색
   - 새로운 관측치와 각각의 학습 데이터 사이의 거리를 전부 측정해야 하므로 계산 시간이 오래 걸리는 한계
